import json
import os
import sys
import gc
import traceback

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.model_selection import StratifiedGroupKFold
from torch.utils.data import DataLoader

# Th√™m ƒë∆∞·ªùng d·∫´n g·ªëc c·ªßa d·ª± √°n ƒë·ªÉ import c√°c module n·ªôi b·ªô
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.utils.losses import FocalLossBCE
from src.data_logic.bcn_dataset import DermoscopyDataset
from src.models import get_model
from src.utils.common import seed_everything, get_warmup_cosine_scheduler, set_finetune_mode
from src.utils.trainer import train_loop
from src.utils.common import save_metadata_info


# ------------------- KI·ªÇM TRA GPU -------------------
def check_gpu_status():
    print("\nüîç --- KI·ªÇM TRA TR·∫†NG TH√ÅI GPU ---")
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        print(f"‚úÖ ƒê√£ t√¨m th·∫•y GPU: {gpu_name}")
        return 'cuda'
    else:
        print("‚ùå KH√îNG T√åM TH·∫§Y GPU! Code s·∫Ω ch·∫°y ch·∫≠m tr√™n CPU.")
        return 'cpu'


# ------------------- CONFIG -------------------
CONFIG = {
    'TRAIN_CSV': r'D:\skin_cancer_project\dataset\metadata\bcn20000_train.csv',
    'VAL_CSV': r'D:\skin_cancer_project\dataset\metadata\bcn20000_val.csv',
    'TEST_CSV': r'D:\skin_cancer_project\dataset\metadata\bcn20000_test.csv',
    'IMG_ROOT': r'D:\skin_cancer_project\dataset\Bcn20000-preprocessed',
    'MODEL_OUT': r'D:\skin_cancer_project\checkpoint_bcn20000',

    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',
    'SEED': 42,

    # --- MODEL---
    'MODEL_NAME': 'resnet50',
    'SHORT_NAME': 'resnet50_bcn',
    'IMG_SIZE': 224,
    'BATCH_SIZE': 32, # 32 l√† m·ª©c an to√†n cho ·∫£nh 224x224 tr√™n h·∫ßu h·∫øt GPU

    'EPOCHS': 20,
    'BASE_LR': 1e-4,
    'WARMUP_EPOCHS': 3,
    'WEIGHT_DECAY': 1e-3,

    # --- METADATA ---
    'METADATA_MODE': 'late_fusion',
    'METADATA_FEATURE_BOOST': 2.0,
    'PRETRAINED': True,
    'FINE_TUNE_MODE': 'partial_unfreeze',

    # C·∫•u tr√∫c unfreeze chu·∫©n cho ResNet
    'UNFREEZE_KEYWORDS': ['layers', 'blocks', 'norm', 'conv_head', 'features', 'stem'],

    'ACCUM_STEPS': 1,

    # --- C·∫§U H√åNH PH√ÇN T√çCH ---
    'ANALYZE_METADATA': True,
    'ANCHOR_VALUE_NAME': 'palms/soles',

    # --- C·∫§U H√åNH GRAD-CAM ---
    'ENABLE_GRAD_CAM': True,
    'GRAD_CAM_FREQ': 5,
}


def preprocess_bcn(df):
    """Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu d·ª±a tr√™n th·ª±c t·∫ø file CSV c·ªßa BCN20000"""
    df = df.copy()
    df.columns = df.columns.str.strip()

    if 'image_path' not in df.columns and 'isic_id' in df.columns:
        df['image_path'] = df['isic_id'].astype(str) + '.jpg'

    # X·ª≠ l√Ω diagnosis ƒë·ªÉ t·∫°o nh√£n
    if 'diagnosis_1' in df.columns:
        df['diagnosis_1'] = df['diagnosis_1'].astype(str).str.strip().str.lower()
        df = df[~df['diagnosis_1'].isin(['nan', '', 'none', 'null'])].copy()
        df['label'] = df['diagnosis_1'].apply(lambda x: 1 if 'malig' in x else 0)
    elif 'diagnosis' in df.columns:
        df['diagnosis'] = df['diagnosis'].astype(str).str.strip().str.lower()
        df = df[~df['diagnosis'].isin(['nan', '', 'none', 'null'])].copy()
        df['label'] = df['diagnosis'].apply(lambda x: 1 if 'malig' in x else 0)

    # --- PH·ª§C H·ªíI CH·ªêT CH·∫∂N: ƒê·∫¢M B·∫¢O C√ì LESION_ID & KH√îNG B·ªä NAN ---
    if 'lesion_id' not in df.columns:
        if 'patient_id' in df.columns:
            df['lesion_id'] = df['patient_id']
        else:
            df['lesion_id'] = df['isic_id'] if 'isic_id' in df.columns else df.index.astype(str)
    df['lesion_id'] = df['lesion_id'].fillna(df['image_path'])

    # --- PH·ª§C H·ªíI CH·ªêT CH·∫∂N: X√ìA C·ªòT ƒê√ÅP √ÅN ƒê·ªÇ CH·ªêNG TARGET LEAKAGE ---
    df = df.drop(columns=['diagnosis', 'diagnosis_1', 'benign_malignant'], errors='ignore')

    return df


def analyze_feature_importance_only(train_df, categorical_cols, numeric_cols, config):
    print(f"\nü§ñ [Analysis] ƒêang ch·∫°y Random Forest ph√¢n t√≠ch Metadata BCN20000...")
    valid_cat = [c for c in categorical_cols if c in train_df.columns]
    valid_num = [c for c in numeric_cols if c in train_df.columns]

    if not valid_cat and not valid_num:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt Metadata h·ª£p l·ªá ƒë·ªÉ ph√¢n t√≠ch.")
        return

    y = train_df['label'].values
    if len(np.unique(y)) < 2:
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ ph√¢n t√≠ch Metadata v√¨ ch·ªâ c√≥ 1 l·ªõp.")
        return

    meta_df = pd.DataFrame()
    if valid_cat:
        temp_cat = train_df[valid_cat].fillna('unknown')
        meta_df = pd.concat([meta_df, pd.get_dummies(temp_cat, columns=valid_cat, prefix_sep='=')], axis=1)

    if valid_num:
        temp_num = train_df[valid_num].copy()
        imputer = SimpleImputer(strategy='mean')
        temp_num_filled = pd.DataFrame(imputer.fit_transform(temp_num), columns=valid_num)
        meta_df = pd.concat([meta_df, temp_num_filled], axis=1)

    rf = RandomForestClassifier(n_estimators=100, max_depth=8, class_weight='balanced', random_state=42, n_jobs=-1)
    rf.fit(meta_df, y)

    feature_names = meta_df.columns
    importances = rf.feature_importances_
    anchor_value = config.get('ANCHOR_VALUE_NAME', 'palms/soles')

    feature_imp_list = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)

    print("\nüìä B·∫¢NG X·∫æP H·∫†NG ƒê·ªò QUAN TR·ªåNG (G·ª£i √Ω t·ª´ RF):")

    anchor_score = 0
    for n, s in feature_imp_list:
        if isinstance(anchor_value, str) and anchor_value in n:
            anchor_score = s
            break

    print(f"   ‚öñÔ∏è  M·ªëc chu·∫©n '{anchor_value}': {anchor_score:.5f}")
    for i, (name, score) in enumerate(feature_imp_list[:10]):
        status = "‚úÖ M·∫†NH" if score >= anchor_score else "‚ö†Ô∏è Y·∫æU"
        print(f"   {i + 1}. {name:<25}: {score:.5f} [{status}]")

    run_dir = config.get('RUN_DIR', config['MODEL_OUT'])
    os.makedirs(run_dir, exist_ok=True)
    pd.DataFrame(feature_imp_list, columns=['Feature', 'Importance']).to_csv(
        os.path.join(run_dir, f"{config['SHORT_NAME']}_meta_importance.csv"), index=False)


def main(config):
    seed_everything(config['SEED'])
    config['DEVICE'] = check_gpu_status()
    device = torch.device(config['DEVICE'])

    # ==========================================================
    # 1. T·∫†O TH∆Ø M·ª§C G·ªêC CHO CROSS-VALIDATION
    # ==========================================================
    cv_run_name = f"CV5_{config['METADATA_MODE']}_{config['SHORT_NAME']}"
    cv_dir = os.path.join(config['MODEL_OUT'], cv_run_name)
    os.makedirs(cv_dir, exist_ok=True)

    print("=" * 50)
    print(f"üìÇ Th∆∞ m·ª•c g·ªëc CV: {cv_dir}")
    print(f"üî• Thi·∫øt b·ªã: {device}")
    print("=" * 50)

    # ==========================================================
    # 2. T·∫¢I V√Ä G·ªòP D·ªÆ LI·ªÜU (TRAIN + VAL)
    # ==========================================================
    print("üìÇ ƒêang t·∫£i d·ªØ li·ªáu BCN20000...")
    raw_train = preprocess_bcn(pd.read_csv(config['TRAIN_CSV']))
    raw_val = preprocess_bcn(pd.read_csv(config['VAL_CSV']))
    raw_test = preprocess_bcn(pd.read_csv(config['TEST_CSV']))

    # G·ªôp Train v√† Val th√†nh 1 t·∫≠p duy nh·∫•t (Development set)
    df_cv = pd.concat([raw_train, raw_val]).reset_index(drop=True)
    print(f"üìä T·ªïng s·ªë m·∫´u ch·∫°y CV (Train+Val): {len(df_cv)}")
    print(f"üìä T·ªïng s·ªë m·∫´u Test (Hold-out): {len(raw_test)}")

    # ==========================================================
    # üõ°Ô∏è KI·ªÇM TRA B·∫¢O M·∫¨T 1: R√í R·ªà TO√ÄN C·ª§C (CV vs TEST)
    # ==========================================================
    if 'lesion_id' in df_cv.columns:
        group_col = 'lesion_id'
    elif 'patient_id' in df_cv.columns:
        group_col = 'patient_id'
    else:
        group_col = 'isic_id'

    if group_col in df_cv.columns and group_col in raw_test.columns:
        cv_ids = set(df_cv[group_col].dropna().unique())
        test_ids = set(raw_test[group_col].dropna().unique())
        leakage = cv_ids.intersection(test_ids)

        if len(leakage) > 0:
            print(f"\n‚ùå [L·ªñI NGHI√äM TR·ªåNG] Ph√°t hi·ªán {len(leakage)} '{group_col}' b·ªã tr√πng l·∫∑p gi·ªØa t·∫≠p CV v√† t·∫≠p Test!")
            print(f"Danh s√°ch ID b·ªã tr√πng (sample): {list(leakage)[:5]}")
            raise ValueError(f"DATA LEAKAGE DETECTED (CV vs TEST). Vui l√≤ng ki·ªÉm tra l·∫°i qu√° tr√¨nh chia file CSV g·ªëc. ƒê√£ d·ª´ng hu·∫•n luy·ªán!")
        else:
            print(f"‚úÖ CH·ªêT CH·∫∂N 1: Tuy·ªát ƒë·ªëi an to√†n. Kh√¥ng c√≥ r√≤ r·ªâ b·ªánh nh√¢n t·ª´ t·∫≠p CV sang t·∫≠p Test.")

    if config.get('ANALYZE_METADATA'):
        categorical_cols = ['sex', 'anatom_site_general']
        numeric_cols = ['age_approx']
        analyze_feature_importance_only(raw_train, categorical_cols, numeric_cols, config)

    # ==========================================================
    # 3. THI·∫æT L·∫¨P STRATIFIED GROUP K-FOLD
    # ==========================================================
    k_folds = 5
    sgkf = StratifiedGroupKFold(n_splits=k_folds, shuffle=True, random_state=config['SEED'])
    fold_results = []

    # ==========================================================
    # 4. V√íNG L·∫∂P HU·∫§N LUY·ªÜN QUA T·ª™NG FOLD
    # ==========================================================
    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X=df_cv, y=df_cv['label'], groups=df_cv[group_col])):
        print(f"\n" + "‚òÖ" * 40)
        print(f"üöÄ B·∫ÆT ƒê·∫¶U FOLD {fold + 1}/{k_folds}")
        print("‚òÖ" * 40)

        fold_dir = os.path.join(cv_dir, f"fold_{fold + 1}")
        os.makedirs(fold_dir, exist_ok=True)
        config['RUN_DIR'] = fold_dir

        fold_train_df = df_cv.iloc[train_idx].reset_index(drop=True)
        fold_val_df = df_cv.iloc[val_idx].reset_index(drop=True)

        # ==========================================================
        # üõ°Ô∏è KI·ªÇM TRA B·∫¢O M·∫¨T 2: R√í R·ªà C·ª§C B·ªò (TRAIN vs VAL)
        # ==========================================================
        train_ids = set(fold_train_df[group_col].dropna().unique())
        val_ids = set(fold_val_df[group_col].dropna().unique())
        fold_leakage = train_ids.intersection(val_ids)

        if len(fold_leakage) > 0:
            raise ValueError(f"‚ùå [L·ªñI NGHI√äM TR·ªåNG] Data Leakage t·∫°i Fold {fold + 1}! GroupKFold ho·∫°t ƒë·ªông kh√¥ng ƒë√∫ng. D·ª´ng ch∆∞∆°ng tr√¨nh!")
        else:
            print(f"   ‚úÖ CH·ªêT CH·∫∂N 2: Fold {fold + 1} an to√†n tuy·ªát ƒë·ªëi (0 ID tr√πng l·∫∑p).")

        # Kh·ªüi t·∫°o Dataset c·ªßa BCN20000 (DermoscopyDataset)
        train_ds = DermoscopyDataset(fold_train_df, config['IMG_ROOT'], config['IMG_SIZE'], config['METADATA_MODE'], train=True)
        val_ds = DermoscopyDataset(fold_val_df, config['IMG_ROOT'], config['IMG_SIZE'], config['METADATA_MODE'], train=False)
        test_ds = DermoscopyDataset(raw_test, config['IMG_ROOT'], config['IMG_SIZE'], config['METADATA_MODE'], train=False)

        # üöÄ G√ÅN B·ªò ENCODER & STATS C·ª¶A TRAIN SANG VAL V√Ä TEST
        val_ds.encoders = train_ds.encoders
        val_ds.num_mean_std = train_ds.num_mean_std

        test_ds.encoders = train_ds.encoders
        test_ds.num_mean_std = train_ds.num_mean_std

        # L∆∞u Encoders c·ªßa fold n√†y ra file
        meta_save_path = os.path.join(fold_dir, f"meta_info_fold{fold + 1}.pkl")
        save_metadata_info(meta_save_path, train_ds.encoders, train_ds.num_mean_std)

        # DataLoader (S·ª≠ d·ª•ng shuffle=True, b·ªè Sampler v√¨ ƒë√£ d√πng FocalLossBCE)
        train_loader = DataLoader(train_ds, batch_size=config['BATCH_SIZE'], shuffle=True, num_workers=4)
        val_loader = DataLoader(val_ds, batch_size=config['BATCH_SIZE'], shuffle=False, num_workers=4)
        test_loader = DataLoader(test_ds, batch_size=config['BATCH_SIZE'], shuffle=False, num_workers=4)

        # Kh·ªüi t·∫°o Model M·ªöI CHO FOLD
        model = get_model(config, train_ds.cat_cardinalities, len(train_ds.numeric_cols)).to(device)
        set_finetune_mode(model, config['FINE_TUNE_MODE'], config.get('UNFREEZE_KEYWORDS', []))

        optimizer = torch.optim.AdamW(model.parameters(), lr=config['BASE_LR'], weight_decay=config['WEIGHT_DECAY'])
        criterion = FocalLossBCE(alpha=0.75, gamma=2.0)
        scheduler = get_warmup_cosine_scheduler(optimizer, config['WARMUP_EPOCHS'], config['EPOCHS'])

        # Ch·∫°y Hu·∫•n luy·ªán
        _, _, test_metrics = train_loop(
            model, train_loader, val_loader, test_loader,
            config, criterion, optimizer, scheduler, device,
            log_suffix=f"fold_{fold + 1}"
        )

        test_metrics['fold'] = fold + 1
        fold_results.append(test_metrics)
        print(f"‚úÖ ƒê√£ xong Fold {fold + 1}. AUC tr√™n t·∫≠p Test: {test_metrics['auc']:.4f}")

    # ==========================================================
    # 5. T·ªîNG H·ª¢P V√Ä IN K·∫æT QU·∫¢ MEAN ¬± STD
    # ==========================================================
    print("\n" + "=" * 50)
    print("üìä B·∫¢NG 1: K·∫æT QU·∫¢ CROSS-VALIDATION MEAN ¬± STD TR√äN T·∫¨P TEST")
    print("=" * 50)

    df_results = pd.DataFrame(fold_results)
    print(df_results[['fold', 'auc', 'acc', 'f1', 'precision', 'recall']])

    summary_data = []
    metrics = ['auc', 'acc', 'f1', 'precision', 'recall']

    print("\nTRUNG B√åNH ¬± ƒê·ªò L·ªÜCH CHU·∫®N:")
    for metric in metrics:
        if metric in df_results.columns:
            mean_val = df_results[metric].mean()
            std_val = df_results[metric].std()

            print(f"{metric.upper():<10} : {mean_val:.4f} ¬± {std_val:.4f}")

            summary_data.append({
                'Metric': metric.upper(),
                'Mean': round(mean_val, 4),
                'Std': round(std_val, 4),
                'Mean_¬±_Std': f"{mean_val:.4f} ¬± {std_val:.4f}"
            })

    df_summary = pd.DataFrame(summary_data)

    detail_csv_path = os.path.join(cv_dir, f"cv5_{config['SHORT_NAME']}_detail_results.csv")
    df_results.to_csv(detail_csv_path, index=False)

    summary_csv_path = os.path.join(cv_dir, f"cv5_{config['SHORT_NAME']}_summary_table1.csv")
    df_summary.to_csv(summary_csv_path, index=False)

    print(f"\nüíæ ƒê√£ l∆∞u chi ti·∫øt t·ª´ng fold t·∫°i  : {detail_csv_path}")
    print(f"üíæ ƒê√£ l∆∞u b·∫£ng T√≥m t·∫Øt (B·∫£ng 1) t·∫°i: {summary_csv_path}")


if __name__ == '__main__':
    modes_to_run = ['diag1', 'full', 'full_weighted', 'late_fusion']

    print("\n" + "‚òÖ" * 60)
    print("üåô CH·∫æ ƒê·ªò CH·∫†Y QUA ƒê√äM (OVERNIGHT TRAINING) BCN20000 ƒê√É K√çCH HO·∫†T")
    print("‚òÖ" * 60 + "\n")

    for mode in modes_to_run:
        try:
            print("\n" + "=" * 60)
            print(f"üöÄ [TI·∫æN TR√åNH] ƒêANG CH·∫†Y MODE: {mode.upper()}")
            print("=" * 60 + "\n")

            CONFIG['METADATA_MODE'] = mode
            main(CONFIG)

            print(f"\n‚úÖ [TH√ÄNH C√îNG] ƒê√É XONG MODE: {mode.upper()}")

        except Exception as e:
            print(f"\n‚ùå [L·ªñI NGHI√äM TR·ªåNG] Mode {mode.upper()} g·∫∑p s·ª± c·ªë!")
            print(f"Chi ti·∫øt l·ªói: {e}")
            traceback.print_exc()
            print("‚è≠Ô∏è ƒêANG CHUY·ªÇN SANG MODE TI·∫æP THEO...\n")

        finally:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print(f"üßπ ƒê√£ d·ªçn d·∫πp b·ªô nh·ªõ GPU.\n")

    print("\n" + "üéâ" * 20)
    print("ƒê√É K·∫æT TH√öC TO√ÄN B·ªò QU√Å TR√åNH HU·∫§N LUY·ªÜN BCN20000!")
    print("üéâ" * 20)