import os
import sys
import gc
import traceback

import numpy as np
import json
import pandas as pd
import torch
from sklearn.model_selection import StratifiedGroupKFold
from torch.utils.data import DataLoader, WeightedRandomSampler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer

# Th√™m ƒë∆∞·ªùng d·∫´n g·ªëc
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.data_logic.ham_dataset import HAM10000Dataset
from src.models import get_model
from src.utils.losses import FocalLossBCE
from src.utils.common import seed_everything, get_warmup_cosine_scheduler, set_finetune_mode
from src.utils.trainer import train_loop
from src.utils.common import save_metadata_info


# ------------------- KI·ªÇM TRA GPU -------------------
def check_gpu_status():
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        print(f"\n‚úÖ ƒê√£ t√¨m th·∫•y GPU: {gpu_name}")
        return 'cuda'
    else:
        print("\n‚ùå KH√îNG T√åM TH·∫§Y GPU! Code s·∫Ω ch·∫°y ch·∫≠m tr√™n CPU.")
        return 'cpu'


# ------------------- CONFIG -------------------
CONFIG = {
    'TRAIN_CSV': r'D:\skin_cancer_project\dataset\metadata\ham10000_train.csv',
    'VAL_CSV': r'D:\skin_cancer_project\dataset\metadata\ham10000_val.csv',
    'TEST_CSV': r'D:\skin_cancer_project\dataset\metadata\ham10000_test.csv',
    'IMG_ROOT': r'D:\skin_cancer_project\dataset\Ham10000-preprocessed',
    'MODEL_OUT': r'D:\skin_cancer_project\checkpoint_ham10000',

    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',
    'SEED': 42,

    # --- MODEL & NAMING ---
    'MODEL_NAME': 'resnet50',
    'SHORT_NAME': 'resnet50_ham',

    'IMG_SIZE': 224,
    'BATCH_SIZE': 32,  # Khuy√™n d√πng 32 ƒë·ªÉ tr√°nh tr√†n RAM
    'EPOCHS': 20,
    'BASE_LR': 8e-5,
    'WARMUP_EPOCHS': 3,
    'WEIGHT_DECAY': 1e-3,

    'METADATA_MODE': 'late_fusion',
    'PRETRAINED': True,
    'FINE_TUNE_MODE': 'partial_unfreeze',
    'UNFREEZE_SUBSTRINGS': ['layers', 'blocks', 'norm', 'conv_head', 'features', 'stem'],

    'USE_SAMPLER': True,
    'ACCUM_STEPS': 1,

    # --- ANALYSIS ---
    'ANALYZE_METADATA': True,
    'ANCHOR_VALUE_NAME': 'lower extremity',

    # --- GRAD-CAM ---
    'ENABLE_GRAD_CAM': True,
    'GRAD_CAM_FREQ': 5,
}


def preprocess_ham(df):
    df = df.copy()
    df.columns = df.columns.str.strip().str.lower()

    if 'image_path' not in df.columns and 'image_id' in df.columns:
        df['image_path'] = df['image_id'].astype(str) + '.jpg'

    # √âp ki·ªÉu age v·ªÅ s·ªë v√† ƒëi·ªÅn gi√° tr·ªã thi·∫øu
    if 'age' in df.columns:
        df['age'] = pd.to_numeric(df['age'], errors='coerce')
        df['age'] = df['age'].fillna(df['age'].mean())

    if 'dx' in df.columns and 'label' not in df.columns:
        # 1: √Åc t√≠nh (mel, bcc, akiec), 0: L√†nh t√≠nh
        df['label'] = df['dx'].apply(lambda x: 1 if x in ['mel', 'bcc', 'akiec'] else 0)

    # --- ƒê·∫¢M B·∫¢O C√ì C·ªòT LESION_ID ƒê·ªÇ CHIA GROUP ---
    if 'lesion_id' not in df.columns:
        df['lesion_id'] = df['image_id'] if 'image_id' in df.columns else df.index.astype(str)
    df['lesion_id'] = df['lesion_id'].fillna(df['image_path'])

    # --- üöÄ CH·ªêT CH·∫∂N: X√ìA C·ªòT ƒê√ÅP √ÅN ƒê·ªÇ CH·ªêNG TARGET LEAKAGE ---
    df = df.drop(columns=['dx', 'dx_type'], errors='ignore')

    return df


# --- PH√ÇN T√çCH QUAN TR·ªåNG (Random Forest) ---
def analyze_feature_importance_only(train_df, categorical_cols, numeric_cols, config):
    print(f"\nü§ñ [Analysis] ƒêang ch·∫°y Random Forest ƒë·ªÉ ƒë√°nh gi√° Metadata HAM10000...")
    valid_cat = [c for c in categorical_cols if c in train_df.columns]
    valid_num = [c for c in numeric_cols if c in train_df.columns]

    if not valid_cat and not valid_num: return

    meta_df = pd.DataFrame()
    if valid_cat:
        temp_cat = train_df[valid_cat].fillna('unknown')
        meta_df = pd.concat([meta_df, pd.get_dummies(temp_cat, columns=valid_cat)], axis=1)
    if valid_num:
        imputer = SimpleImputer(strategy='mean')
        temp_num = pd.DataFrame(imputer.fit_transform(train_df[valid_num]), columns=valid_num)
        meta_df = pd.concat([meta_df, temp_num], axis=1)

    y = train_df['label'].values
    rf = RandomForestClassifier(n_estimators=100, max_depth=8, class_weight='balanced', random_state=42, n_jobs=-1)
    rf.fit(meta_df, y)

    imps = sorted(zip(meta_df.columns, rf.feature_importances_), key=lambda x: x[1], reverse=True)

    anchor_score = next((s for n, s in imps if config['ANCHOR_VALUE_NAME'] in n), 0)
    print("\nüìä TOP METADATA FEATURES:")
    for i, (name, score) in enumerate(imps[:8]):
        status = "‚úÖ M·∫†NH" if score > anchor_score else "‚ö†Ô∏è Y·∫æU"
        print(f"   {i + 1}. {name}: {score:.5f} [{status}]")

    run_dir = config.get('RUN_DIR', config['MODEL_OUT'])
    os.makedirs(run_dir, exist_ok=True)
    csv_name = f"ham10k_{config['SHORT_NAME']}_meta_imp.csv"
    out_path = os.path.join(run_dir, csv_name)

    pd.DataFrame(imps, columns=['Feature', 'Importance']).to_csv(out_path, index=False)
    print(f"üíæ ƒê√£ l∆∞u b·∫£ng x·∫øp h·∫°ng Metadata v√†o: {out_path}")


def main(config):
    seed_everything(config['SEED'])
    config['DEVICE'] = check_gpu_status()
    device = torch.device(config['DEVICE'])

    # ==========================================================
    # 1. T·∫†O TH∆Ø M·ª§C G·ªêC CHO CROSS-VALIDATION
    # ==========================================================
    cv_run_name = f"CV5_{config['METADATA_MODE']}_{config['SHORT_NAME']}"
    cv_dir = os.path.join(config['MODEL_OUT'], cv_run_name)
    os.makedirs(cv_dir, exist_ok=True)

    print("=" * 50)
    print(f"üìÇ Th∆∞ m·ª•c g·ªëc CV: {cv_dir}")
    print(f"üî• Thi·∫øt b·ªã: {device}")
    print("=" * 50)

    # ==========================================================
    # 2. T·∫¢I V√Ä G·ªòP D·ªÆ LI·ªÜU (TRAIN + VAL)
    # ==========================================================
    print("üìÇ ƒêang t·∫£i d·ªØ li·ªáu HAM10000...")
    raw_train = preprocess_ham(pd.read_csv(config['TRAIN_CSV']))
    raw_val = preprocess_ham(pd.read_csv(config['VAL_CSV']))
    raw_test = preprocess_ham(pd.read_csv(config['TEST_CSV']))

    # G·ªôp Train v√† Val th√†nh 1 t·∫≠p duy nh·∫•t (Development set)
    df_cv = pd.concat([raw_train, raw_val]).reset_index(drop=True)
    print(f"üìä T·ªïng s·ªë m·∫´u ch·∫°y CV (Train+Val): {len(df_cv)}")
    print(f"üìä T·ªïng s·ªë m·∫´u Test (Hold-out): {len(raw_test)}")

    # ==========================================================
    # üõ°Ô∏è KI·ªÇM TRA B·∫¢O M·∫¨T 1: R√í R·ªà TO√ÄN C·ª§C (CV vs TEST)
    # ==========================================================
    group_col = 'lesion_id' if 'lesion_id' in df_cv.columns else 'image_id'

    if group_col in df_cv.columns and group_col in raw_test.columns:
        cv_ids = set(df_cv[group_col].dropna().unique())
        test_ids = set(raw_test[group_col].dropna().unique())
        leakage = cv_ids.intersection(test_ids)

        if len(leakage) > 0:
            print(
                f"\n‚ùå [L·ªñI NGHI√äM TR·ªåNG] Ph√°t hi·ªán {len(leakage)} '{group_col}' b·ªã tr√πng l·∫∑p gi·ªØa t·∫≠p CV v√† t·∫≠p Test!")
            print(f"Danh s√°ch ID b·ªã tr√πng (sample): {list(leakage)[:5]}")
            raise ValueError(
                f"DATA LEAKAGE DETECTED (CV vs TEST). Vui l√≤ng ki·ªÉm tra l·∫°i qu√° tr√¨nh chia file CSV g·ªëc. ƒê√£ d·ª´ng hu·∫•n luy·ªán!")
        else:
            print(f"‚úÖ CH·ªêT CH·∫∂N 1: Tuy·ªát ƒë·ªëi an to√†n. Kh√¥ng c√≥ r√≤ r·ªâ b·ªánh nh√¢n t·ª´ t·∫≠p CV sang t·∫≠p Test.")

    if config.get('ANALYZE_METADATA'):
        categorical_cols = ['sex', 'localization']
        numeric_cols = ['age']
        analyze_feature_importance_only(raw_train, categorical_cols, numeric_cols, config)

    # ==========================================================
    # 3. THI·∫æT L·∫¨P STRATIFIED GROUP K-FOLD
    # ==========================================================
    k_folds = 5
    sgkf = StratifiedGroupKFold(n_splits=k_folds, shuffle=True, random_state=config['SEED'])
    fold_results = []

    # ==========================================================
    # 4. V√íNG L·∫∂P HU·∫§N LUY·ªÜN QUA T·ª™NG FOLD
    # ==========================================================
    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X=df_cv, y=df_cv['label'], groups=df_cv[group_col])):
        print(f"\n" + "‚òÖ" * 40)
        print(f"üöÄ B·∫ÆT ƒê·∫¶U FOLD {fold + 1}/{k_folds}")
        print("‚òÖ" * 40)

        fold_dir = os.path.join(cv_dir, f"fold_{fold + 1}")
        os.makedirs(fold_dir, exist_ok=True)
        config['RUN_DIR'] = fold_dir

        fold_train_df = df_cv.iloc[train_idx].reset_index(drop=True)
        fold_val_df = df_cv.iloc[val_idx].reset_index(drop=True)

        # ==========================================================
        # üõ°Ô∏è KI·ªÇM TRA B·∫¢O M·∫¨T 2: R√í R·ªà C·ª§C B·ªò (TRAIN vs VAL)
        # ==========================================================
        train_ids = set(fold_train_df[group_col].dropna().unique())
        val_ids = set(fold_val_df[group_col].dropna().unique())
        fold_leakage = train_ids.intersection(val_ids)

        if len(fold_leakage) > 0:
            raise ValueError(
                f"‚ùå [L·ªñI NGHI√äM TR·ªåNG] Data Leakage t·∫°i Fold {fold + 1}! GroupKFold ho·∫°t ƒë·ªông kh√¥ng ƒë√∫ng. D·ª´ng ch∆∞∆°ng tr√¨nh!")
        else:
            print(f"   ‚úÖ CH·ªêT CH·∫∂N 2: Fold {fold + 1} an to√†n tuy·ªát ƒë·ªëi (0 ID tr√πng l·∫∑p).")

        # Kh·ªüi t·∫°o Dataset c·ªßa HAM10000
        train_ds = HAM10000Dataset(fold_train_df, config['IMG_ROOT'], config['IMG_SIZE'], config['METADATA_MODE'],
                                   train=True)
        val_ds = HAM10000Dataset(fold_val_df, config['IMG_ROOT'], config['IMG_SIZE'], config['METADATA_MODE'],
                                 train=False)
        test_ds = HAM10000Dataset(raw_test, config['IMG_ROOT'], config['IMG_SIZE'], config['METADATA_MODE'],
                                  train=False)

        # üöÄ G√ÅN B·ªò ENCODER & STATS C·ª¶A TRAIN SANG VAL V√Ä TEST
        val_ds.encoders = train_ds.encoders
        val_ds.num_mean_std = train_ds.num_mean_std

        test_ds.encoders = train_ds.encoders
        test_ds.num_mean_std = train_ds.num_mean_std

        # L∆∞u Encoders c·ªßa fold n√†y ra file
        meta_save_path = os.path.join(fold_dir, f"meta_info_fold{fold + 1}.pkl")
        save_metadata_info(meta_save_path, train_ds.encoders, train_ds.num_mean_std)

        # C·∫•u h√¨nh Sampler cho m·∫•t c√¢n b·∫±ng d·ªØ li·ªáu
        train_sampler = None
        if config.get('USE_SAMPLER', False):
            targets = fold_train_df['label'].values
            class_counts = np.bincount(targets)
            weights = 1. / (class_counts + 1e-6)
            samples_weights = torch.from_numpy(weights[targets]).double()
            train_sampler = WeightedRandomSampler(samples_weights, len(samples_weights))

        # DataLoaders
        train_loader = DataLoader(train_ds, batch_size=config['BATCH_SIZE'], sampler=train_sampler,
                                  shuffle=(train_sampler is None), num_workers=4)
        val_loader = DataLoader(val_ds, batch_size=config['BATCH_SIZE'], shuffle=False, num_workers=4)
        test_loader = DataLoader(test_ds, batch_size=config['BATCH_SIZE'], shuffle=False, num_workers=4)

        # Kh·ªüi t·∫°o Model M·ªöI CHO FOLD
        model = get_model(config, train_ds.cat_cardinalities, len(train_ds.numeric_cols)).to(device)
        set_finetune_mode(model, config['FINE_TUNE_MODE'], config.get('UNFREEZE_SUBSTRINGS', []))

        optimizer = torch.optim.AdamW(model.parameters(), lr=config['BASE_LR'], weight_decay=config['WEIGHT_DECAY'])
        criterion = FocalLossBCE(alpha=0.75, gamma=2.0)
        scheduler = get_warmup_cosine_scheduler(optimizer, config['WARMUP_EPOCHS'], config['EPOCHS'])

        # Ch·∫°y Hu·∫•n luy·ªán
        _, _, test_metrics = train_loop(
            model, train_loader, val_loader, test_loader,
            config, criterion, optimizer, scheduler, device,
            log_suffix=f"fold_{fold + 1}"
        )

        test_metrics['fold'] = fold + 1
        fold_results.append(test_metrics)
        print(f"‚úÖ ƒê√£ xong Fold {fold + 1}. AUC tr√™n t·∫≠p Test: {test_metrics['auc']:.4f}")

    # ==========================================================
    # 5. T·ªîNG H·ª¢P V√Ä IN K·∫æT QU·∫¢ MEAN ¬± STD
    # ==========================================================
    print("\n" + "=" * 50)
    print("üìä B·∫¢NG 1: K·∫æT QU·∫¢ CROSS-VALIDATION MEAN ¬± STD TR√äN T·∫¨P TEST")
    print("=" * 50)

    df_results = pd.DataFrame(fold_results)
    print(df_results[['fold', 'auc', 'acc', 'f1', 'precision', 'recall']])

    summary_data = []
    metrics = ['auc', 'acc', 'f1', 'precision', 'recall']

    print("\nTRUNG B√åNH ¬± ƒê·ªò L·ªÜCH CHU·∫®N:")
    for metric in metrics:
        if metric in df_results.columns:
            mean_val = df_results[metric].mean()
            std_val = df_results[metric].std()

            print(f"{metric.upper():<10} : {mean_val:.4f} ¬± {std_val:.4f}")

            summary_data.append({
                'Metric': metric.upper(),
                'Mean': round(mean_val, 4),
                'Std': round(std_val, 4),
                'Mean_¬±_Std': f"{mean_val:.4f} ¬± {std_val:.4f}"
            })

    df_summary = pd.DataFrame(summary_data)

    detail_csv_path = os.path.join(cv_dir, f"cv5_{config['SHORT_NAME']}_detail_results.csv")
    df_results.to_csv(detail_csv_path, index=False)

    summary_csv_path = os.path.join(cv_dir, f"cv5_{config['SHORT_NAME']}_summary_table1.csv")
    df_summary.to_csv(summary_csv_path, index=False)

    print(f"\nüíæ ƒê√£ l∆∞u chi ti·∫øt t·ª´ng fold t·∫°i  : {detail_csv_path}")
    print(f"üíæ ƒê√£ l∆∞u b·∫£ng T√≥m t·∫Øt (B·∫£ng 1) t·∫°i: {summary_csv_path}")


if __name__ == '__main__':
    modes_to_run = ['diag1', 'full', 'full_weighted', 'late_fusion']

    print("\n" + "‚òÖ" * 60)
    print("üåô CH·∫æ ƒê·ªò CH·∫†Y QUA ƒê√äM (OVERNIGHT TRAINING) HAM10000 ƒê√É K√çCH HO·∫†T")
    print("‚òÖ" * 60 + "\n")

    for mode in modes_to_run:
        try:
            print("\n" + "=" * 60)
            print(f"üöÄ [TI·∫æN TR√åNH] ƒêANG CH·∫†Y MODE: {mode.upper()}")
            print("=" * 60 + "\n")

            CONFIG['METADATA_MODE'] = mode
            main(CONFIG)

            print(f"\n‚úÖ [TH√ÄNH C√îNG] ƒê√É XONG MODE: {mode.upper()}")

        except Exception as e:
            print(f"\n‚ùå [L·ªñI NGHI√äM TR·ªåNG] Mode {mode.upper()} g·∫∑p s·ª± c·ªë!")
            print(f"Chi ti·∫øt l·ªói: {e}")
            traceback.print_exc()
            print("‚è≠Ô∏è ƒêANG CHUY·ªÇN SANG MODE TI·∫æP THEO...\n")

        finally:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print(f"üßπ ƒê√£ d·ªçn d·∫πp b·ªô nh·ªõ GPU.\n")

    print("\n" + "üéâ" * 20)
    print("ƒê√É K·∫æT TH√öC TO√ÄN B·ªò QU√Å TR√åNH HU·∫§N LUY·ªÜN HAM10000!")
    print("üéâ" * 20)