import sys
import os
import torch
import numpy as np
import cv2
import matplotlib.pyplot as plt
from torchvision import transforms
from PIL import Image
from lime import lime_image
from skimage.segmentation import mark_boundaries

# Setup path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from src.models import get_model

# C·∫§U H√åNH LOAD MODEL (Ph·∫£i kh·ªõp l√∫c train)
CONFIG = {
    'MODEL_NAME': 'resnet50',        # B·∫ÆT BU·ªòC: ƒê·ªÉ g·ªçi ƒë√∫ng backbone
    'IMG_SIZE': 384,                 # B·∫ÆT BU·ªòC: ƒê·ªÉ resize ·∫£nh ƒë·∫ßu v√†o cho kh·ªõp
    'METADATA_MODE': 'full_weighted',# B·∫ÆT BU·ªòC: ƒê·ªÉ model kh·ªüi t·∫°o ƒë√∫ng Fusion Head
    'METADATA_FEATURE_BOOST': 5.0,   # B·∫ÆT BU·ªòC: Kh·ªõp tham s·ªë boost l√∫c train
    'PRETRAINED': False,             # N√™n ƒë·ªÉ False cho nhanh (v√¨ ta s·∫Ω load weight c·ªßa m√¨nh ƒë√® l√™n)
    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu'
}
CKPT_PATH = '/mnt/d/skin_cancer_project/checkpoint_ResNet50_ham10000/best_full_weighted.pt'
IMG_PATH = '/mnt/d/skin_cancer_project/dataset/Bcn10000-preprocessed/clean_ISIC_0033158.jpg'  # Thay ·∫£nh test v√†o ƒë√¢y


def run_lime():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # 1. Load Model
    # Fake th√¥ng s·ªë metadata ƒë·ªÉ kh·ªüi t·∫°o khung model
    dummy_cats = {'age': 15, 'sex': 3, 'loc': 8}  # V√≠ d·ª•
    dummy_num = 1
    model = get_model(CONFIG, dummy_cats, dummy_num).to(device)

    checkpoint = torch.load(CKPT_PATH, map_location=device)
    model.load_state_dict(checkpoint['state_dict'])
    model.eval()

    # 2. H√†m d·ª± ƒëo√°n cho LIME (Input: Numpy array ·∫£nh)
    def batch_predict(images):
        # LIME ƒë∆∞a v√†o list ·∫£nh numpy (H,W,C) -> C·∫ßn chuy·ªÉn v·ªÅ Tensor (B,C,H,W)
        batch_tensors = []
        for img in images:
            # Chu·∫©n h√≥a gi·ªëng l√∫c train
            img_pil = Image.fromarray(img.astype('uint8'))
            t = transforms.Compose([
                transforms.Resize((CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE'])),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ])
            batch_tensors.append(t(img_pil))

        batch = torch.stack(batch_tensors).to(device)

        # Fake metadata (batch size t∆∞∆°ng ·ª©ng)
        B = batch.size(0)
        meta_num = torch.zeros((B, dummy_num)).to(device)
        meta_cat = torch.zeros((B, len(dummy_cats)), dtype=torch.long).to(device)

        with torch.no_grad():
            logits = model(batch, meta_num, meta_cat)
            probs = torch.sigmoid(logits)

        # LIME c·∫ßn output shape (N_samples, 2) cho b√†i to√°n Binary
        # probs hi·ªán t·∫°i l√† (N, 1) -> ta t·∫°o (N, 2) v·ªõi [1-p, p]
        probs = probs.cpu().numpy()
        return np.hstack([1 - probs, probs])

    # 3. Ch·∫°y LIME
    print("üçã ƒêang kh·ªüi ch·∫°y LIME (S·∫Ω m·∫•t v√†i ph√∫t)...")
    explainer = lime_image.LimeImageExplainer()

    # ƒê·ªçc ·∫£nh g·ªëc ƒë·ªÉ gi·∫£i th√≠ch
    original_img = np.array(Image.open(IMG_PATH).convert('RGB').resize((CONFIG['IMG_SIZE'], CONFIG['IMG_SIZE'])))

    explanation = explainer.explain_instance(
        original_img,
        batch_predict,
        top_labels=1,
        hide_color=0,
        num_samples=1000  # TƒÉng l√™n n·∫øu mu·ªën ch√≠nh x√°c h∆°n
    )

    # 4. Hi·ªÉn th·ªã v√† L∆∞u
    temp, mask = explanation.get_image_and_mask(
        explanation.top_labels[0],
        positive_only=True,
        num_features=5,
        hide_rest=False
    )

    img_boundry = mark_boundaries(temp / 255.0, mask)  # Normalize v·ªÅ 0-1 ƒë·ªÉ hi·ªÉn th·ªã

    plt.figure(figsize=(8, 8))
    plt.imshow(img_boundry)
    plt.title(f"LIME Explanation: Why Malignant?")
    plt.axis('off')
    save_file = 'lime_result.png'
    plt.savefig(save_file, bbox_inches='tight')
    print(f"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ LIME v√†o {save_file}")


if __name__ == '__main__':
    run_lime()